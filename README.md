**Обязательные задания**

1. Вас пригласили настроить мониторинг на проект. На онбординге вам рассказали, что проект представляет из себя платформу для вычислений с выдачей текстовых отчетов, которые сохраняются на диск. Взаимодействие с платформой осуществляется по протоколу http. Также вам отметили, что вычисления загружают ЦПУ. Какой минимальный набор метрик вы выведите в мониторинг и почему?

2. Менеджер продукта посмотрев на ваши метрики сказал, что ему непонятно что такое RAM/inodes/CPUla. Также он сказал, что хочет понимать, насколько мы выполняем свои обязанности перед клиентами и какое качество обслуживания. Что вы можете ему предложить?

3. Вашей DevOps команде в этом году не выделили финансирование на построение системы сбора логов. Разработчики в свою очередь хотят видеть все ошибки, которые выдают их приложения. Какое решение вы можете предпринять в этой ситуации, чтобы разработчики получали ошибки приложения?

4. Вы, как опытный SRE, сделали мониторинг, куда вывели отображения выполнения SLA=99% по http кодам ответов. Вычисляете этот параметр по следующей формуле: summ_2xx_requests/summ_all_requests. Данный параметр не поднимается выше 70%, но при этом в вашей системе нет кодов ответа 5xx и 4xx. Где у вас ошибка?

5. Опишите основные плюсы и минусы pull и push систем мониторинга.

6. Какие из ниже перечисленных систем относятся к push модели, а какие к pull? А может есть гибридные?

 - Prometheus
 - TICK
 - Zabbix
 - VictoriaMetrics
 - Nagios

7. Склонируйте себе репозиторий и запустите TICK-стэк, используя технологии docker и docker-compose.
В виде решения на это упражнение приведите скриншот веб-интерфейса ПО chronograf (http://localhost:8888).

P.S.: если при запуске некоторые контейнеры будут падать с ошибкой - проставьте им режим Z, например ./data:/var/lib:Z

8. Перейдите в веб-интерфейс Chronograf (http://localhost:8888) и откройте вкладку Data explorer.

- Нажмите на кнопку Add a query

- Изучите вывод интерфейса и выберите БД telegraf.autogen

 - В measurments выберите cpu->host->telegraf-getting-started, а в fields выберите usage_system. Внизу появится график утилизации cpu.

-  Вверху вы можете увидеть запрос, аналогичный SQL-синтаксису. Поэкспериментируйте с запросом, попробуйте изменить группировку и интервал наблюдений.

Для выполнения задания приведите скриншот с отображением метрик утилизации cpu из веб-интерфейса.

9. Изучите список telegraf inputs. Добавьте в конфигурацию telegraf следующий плагин - docker:
```
[[inputs.docker]]
  endpoint = "unix:///var/run/docker.sock"
```

Дополнительно вам может потребоваться донастройка контейнера telegraf в docker-compose.yml дополнительного volume и режима privileged:

```
  telegraf:
    image: telegraf:1.4.0
    privileged: true
    volumes:
      - ./etc/telegraf.conf:/etc/telegraf/telegraf.conf:Z
      - /var/run/docker.sock:/var/run/docker.sock:Z
    links:
      - influxdb
    ports:
      - "8092:8092/udp"
      - "8094:8094"
      - "8125:8125/udp"
```
После настройке перезапустите telegraf, обновите веб интерфейс и приведите скриншотом список measurments в веб-интерфейсе базы telegraf.autogen . Там должны появиться метрики, связанные с docker.

Факультативно можете изучить какие метрики собирает telegraf после выполнения данного задания.





**Решение 1**

*Загрузка процессора (CPU) Вычисления загружают ЦПУ, мониторинг использования процессора является приоритетным. Это поможет отслеживать, не приближается ли система к пределам своих возможностей.*

*Метрики:*

 - Средняя загрузка ЦПУ (в процентах).

 - Загрузка по ядрам.

 - Количество процессов, ожидающих ЦПУ (load average).

 - Температура процессора и датчиков системы: полезно для серверов, особенно при высоких вычислительных нагрузках.


*Использование памяти (RAM) Нагрузки в процессе вычислений могут потреблять много оперативной памяти. Недостаток памяти может привести к использованию swap'а или сбоям в работе платформы.*

*Метрики:*

 - Использование оперативной памяти (в абсолютных значениях и процентах).

 - Объем использованного swap (индикатор нехватки памяти).


*Доступность и время отклика HTTP-сервера Взаимодействие с платформой осуществляется по протоколу HTTP, поэтому важно отслеживать ее доступность и время отклика, чтобы удостовериться, что она отвечает на запросы.*

*Метрики:*

 - Время ответа на HTTP-запросы (latency).

 - Статус-коды HTTP (чтобы отслеживать ошибки 4xx и 5xx).

 - Количество активных соединений.


*Дисковое пространство и I/O операции Важно следить за достаточностью дискового пространства, отчеты созраняются на диск. Также стоит отслеживать активность дисковых операций, так как интенсивный I/O может снижать общую производительность.*

*Метрики:*

 - Доступное дисковое пространство.

 - Нагрузка на диск (операции чтения/записи в секунду).

 - Время отклика диска (disk latency).


*Ошибки в работе приложения Ошибки в работе приложения могут указывать на проблемы в логике вычислений или интеграции. Логирование ошибок поможет оперативно выявлять и исправлять неполадки.*

*Метрики:*

 - Логирование критических ошибок или исключений (по уровням log severity).

Дополнительно: Нагрузка на сеть: если результаты вычислений передаются по сети, можно отслеживать объем передаваемых данных и активность сети.





**Решение 2**


RAM - Память, используемая для временного хранения данных и выполнения программ. inodes - Структуры данных в файловой системе, которые хранят информацию о файлах и каталогах. Ограниченное количество inodes может помешать созданию новых файлов CPUla - Загрузка ЦПУ показывает, насколько сильно он занят обработкой задач

Для того чтобы сделать метрики более понятными и удобными для анализа с точки зрения качества обслуживания и выполнения обязательств перед клиентами, можно использовать следующие подходы, связав их с понятиями SLA (Service Level Agreement), SLO (Service Level Objective) и SLI (Service Level Indicator)

*Определение ключевых показателей:*

SLA (Service Level Agreement): это соглашение, определяющее уровень услуг, которые клиент ожидает от провайдера. Например, "Платформа должна быть доступна 99.9% времени". SLO (Service Level Objective): это конкретные цели, которые команда ставит для достижения SLA. Например, "Время отклика на HTTP-запросы должно составлять не более 200 мс для 95% запросов". SLI (Service Level Indicator): это метрики, которые используются для измерения достижения SLO. Например: Доступность: процент времени, когда сервис был доступен. Время отклика: среднее время отклика на HTTP-запросы.

*Перевод технических метрик в понятные клиенту показатели*

Доступность платформы (Uptime) SLI: Процент времени, когда платформа была доступна. SLO: "99.9% доступности". Пояснение: платформа была доступна на 99.9% времени в прошлом месяце.

Время отклика на запросы

SLI: Среднее время отклика на HTTP-запросы. SLO: "90% запросов обрабатываются за менее чем 200 мс". Пояснение: 90% запросов обрабатывались быстрее, чем за 200 мс.

Число ошибок

SLI: Количество 4xx и 5xx ошибок за определённый период. SLO: "Количество ошибок не превышает 1% всех запросов". Пояснение: Ошибки при обработке запросов составляют менее 1% от общего числа.

Что упростит:

 - Создайте регулярные отчёты о выполнении SLO и состоянии сервисов. Это поможет менеджеру продукта видеть прогресс и принимать обоснованные решения.

- Поддерживайте открытый диалог с менеджером продукта, чтобы вместе определять, какие метрики наиболее важны для клиентов, и при необходимости корректировать SLO и SLI в соответствии с изменениями в бизнесе

 - Используйте простые и интуитивно понятные термины. Например, вместо "использование RAM" можно говорить "объём памяти, доступный для обработки ваших запросов"

 - Предоставляйте данные в виде графиков, диаграмм и дашбордов, чтобы менеджер продукта мог легко визуализировать информацию



**Решение 3**

 1). Использование встроенных логов. Логирвоание ошибок в стандартный вывод (stdout) и стандартный вывод ошибок (stderr). Это позволяет легко собирать логи через консоль или оболочку

 2). Инструменты для простого логирования. systemd или journalctl: Если ваши приложения работают на системах с systemd, можно использовать journalctl для просмотра логов. Использование rsyslog.

 3). Скрипты для агрегации логов. Скрипты на Bash или Python, которые будут периодически собирать ошибки из логов и отправлять их по электронной почте разработчикам. Скрипт может запускаться через cron, чтобы проверять файлы логов и отправлять уведомления о новых ошибках.
 4). Использование мессенджеров или уведомлений.Чат-боты, Телеграм, Slack, Discord
 
 5). Логирование в базу данных. Если у вас есть доступ к базам данных, можно записывать ошибки в таблицу. Это позволяет разработчикам выполнять SQL-запросы для извлечения информации об ошибках.

 6). Использование сервиса Sentry, который позволяет удаленно мониторить баги в фронтенд-приложениях, написанных на JavaScript.

 7). Виртуальные окружения для разработки. Если разработчики работают в локальных окружениях, убедитесь, что они могут видеть логи прямо в своих средах, например, используя docker logs для контейнеризованных приложений.



 **Решение 4**

 Проблема связана с тем, что в системе присутствуют HTTP-коды ответов, которые не относятся ни к 2xx, ни к 4xx, ни к 5xx, и они не учитываются в SLA. HTTP-коды в диапазоне 1xx (информационные ответы) и 3xx (перенаправления) также могут присутствовать, и они могут снижать процентное соотношение успешных (2xx) запросов. Например, если в системе много перенаправлений (3xx-коды), они будут включены в общее количество запросов, что понизит метрику SLA, даже если нет ошибок 4xx и 5xx.


Необходимо: Учесть коды 1xx и 3xx в расчете. Пересмотреть формулу, чтобы исключить из общего количества запросов ответы с кодами 1xx и 3xx, если они не должны влиять на SLA. Верная формула: SLA = summ_2xx_requests / (summ_2xx_requests + summ_4xx_requests + summ_5xx_requests) - формула с исключением 1хх и 3хх Таким образом, вы будете рассчитывать SLA только по тем кодам, которые прямо влияют на качество обслуживания (успешные и ошибочные ответы).



**Решение 5**

*Pull система мониторинга*

Принцип работы: центральный сервер (мониторинг-сервер) инициирует запросы к каждому узлу (например, к агенту на сервере) и собирает метрики.

*Плюсы:*

Централизованное управление: Все запросы отправляются из одной точки, что упрощает настройку и контроль. Мониторинг-сервер контролирует, какие метрики собираются и с какой частотой.

Безопасность: Клиенты (сервера, с которых собираются метрики) могут быть настроены таким образом, что они просто отвечают на запросы, не отправляя данные. Это снижает риск утечек. Мониторинг-сервер инициирует все соединения, что уменьшает количество открытых портов на серверах для входящих соединений.

Надежность: Если один из клиентов падает или становится недоступным, мониторинг-сервер это легко обнаружит (метрики перестанут собираться).

Гибкость: Легче добавить новые узлы: мониторинг-сервер просто начинает опрашивать их. Легче управлять частотой сбора метрик с различных узлов.

*Минусы:*

Масштабируемость: По мере увеличения числа узлов и объема данных мониторинг-сервер может стать узким местом. Сервер может испытывать задержки при сборе метрик с большого количества клиентов.

Сложность настройки брандмауэра: Мониторинг-сервер должен иметь доступ к каждому узлу через брандмауэры и сети, что может потребовать дополнительной настройки.

Сложность обработки короткоживущих процессов: Метрики собираются только в момент запроса, поэтому процессы, которые завершаются между запросами, могут быть пропущены.



*Push система мониторинга*

Принцип работы: узлы (агенты на серверах) сами отправляют данные на центральный сервер с метриками (мониторинг-сервер).

*Плюсы:*

1.Масштабируемость: Легче масштабировать на большое количество узлов, так как каждый узел отправляет данные самостоятельно, а нагрузка на мониторинг-сервер распределена по времени.

2.Мониторинг короткоживущих процессов: Метрики отправляются с узлов по мере их появления, что позволяет отслеживать короткоживущие процессы более эффективно.

Прохождение брандмауэров: Серверы могут отправлять данные через существующие каналы связи (например, HTTPS), что упрощает настройку брандмауэров, так как соединения инициируются клиентами.

Низкая зависимость от центрального сервера: Даже если центральный сервер на время недоступен, агенты могут продолжать собирать и отправлять метрики, когда сервер восстановится.

*Минусы:*

1.Безопасность: Необходимость открывать входящие порты на мониторинг-сервере, что увеличивает потенциальные риски (например, DDoS-атаки). Метрики передаются от клиентов, и при неправильной настройке это может привести к утечке данных.

Сложность управления: Труднее централизованно управлять частотой отправки данных и количеством собираемых метрик с каждого узла. Необходимо конфигурировать каждый узел для отправки метрик на центральный сервер.
3.Обнаружение недоступных узлов: Если узел "умолкает" (перестаёт отправлять данные), это не всегда легко заметить. Потребуется настраивать дополнительные механизмы проверки доступности агентов.


**Решение 6**

 - Prometheus	Pull - (Push с Pushgateway)

 - TICK	- (Push)
 
 - Zabbix -	Push, (Pull с Zabbix Proxy)

 - VictoriaMetrics	- Push/Pull (зависит от источника)

 - Nagios -	(Pull)

**Решение 7**

Развернул TICK через docker-compose

Заработало все далеко не с первого раза

Пришлось сделать сначала вот так

```
git clone https://github.com/influxdata/sandbox.git
chown -R root:root sandbox
chmod -R 1777 sandbox
```
после изменить docker-compose файл и добавит туда Z

```
version: '3'
services:
  influxdb:
    # Full tag list: https://hub.docker.com/r/library/influxdb/tags/
    build:
      context: ./images/influxdb/
      dockerfile: ./${TYPE}/Dockerfile
      args:
        INFLUXDB_TAG: ${INFLUXDB_TAG}
    image: "influxdb"
    volumes:
      # Mount for influxdb data directory
      - ./influxdb/data:/var/lib/influxdb:Z
      # Mount for influxdb configuration
      - ./influxdb/config/:/etc/influxdb/:Z
    ports:
      # The API for InfluxDB is served on port 8086
      - "8086:8086"
      - "8082:8082"
      # UDP Port
      - "8089:8089/udp"

  telegraf:
    # Full tag list: https://hub.docker.com/r/library/telegraf/tags/
    build:
      context: ./images/telegraf/
      dockerfile: ./${TYPE}/Dockerfile
      args:
        TELEGRAF_TAG: ${TELEGRAF_TAG}
    image: "telegraf"
    environment:
      HOSTNAME: "telegraf-getting-started"
    # Telegraf requires network access to InfluxDB
    links:
      - influxdb
    volumes:
      # Mount for telegraf configuration
      - ./telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:Z
      # Mount for Docker API access
      - /var/run/docker.sock:/var/run/docker.sock:Z
    depends_on:
      - influxdb

  kapacitor:
  # Full tag list: https://hub.docker.com/r/library/kapacitor/tags/
    build:
      context: ./images/kapacitor/
      dockerfile: ./${TYPE}/Dockerfile
      args:
        KAPACITOR_TAG: ${KAPACITOR_TAG}
    image: "kapacitor"
    volumes:
      # Mount for kapacitor data directory
      - ./kapacitor/data/:/var/lib/kapacitor:Z
      # Mount for kapacitor configuration
      - ./kapacitor/config/:/etc/kapacitor/:Z
    # Kapacitor requires network access to Influxdb
    links:
      - influxdb
    ports:
      # The API for Kapacitor is served on port 9092
      - "9092:9092"

  chronograf:
    # Full tag list: https://hub.docker.com/r/library/chronograf/tags/
    build:
      context: ./images/chronograf
      dockerfile: ./${TYPE}/Dockerfile
      args:
        CHRONOGRAF_TAG: ${CHRONOGRAF_TAG}
    image: "chrono_config"
    environment:
      RESOURCES_PATH: "/usr/share/chronograf/resources"
    volumes:
      # Mount for chronograf database
      - ./chronograf/data/:/var/lib/chronograf/:Z
    links:
      # Chronograf requires network access to InfluxDB and Kapacitor
      - influxdb
      - kapacitor
    ports:
      # The WebUI for Chronograf is served on port 8888
      - "8888:8888"
    depends_on:
      - kapacitor
      - influxdb
      - telegraf

  documentation:
    build:
      context: ./documentation
    ports:
      - "3010:3000"
```

и потом сделать 

```
./sandbox up
```

и после всех манипуляций, О ЧУДО!!!!! у нас завелись все контенеры и  работает веб интерфейс, и даже показывает верную информацию

![alt text](https://github.com/mezhibo/monitoring-system/blob/fc3245198b6bd152a5f5e3be8775f0abead727d6/IMG/1.jpg)




**Решение 8**


Создадим запрос согласно заданию

```
- В measurments выберите cpu->host->telegraf-getting-started, а в fields выберите usage_system. Внизу появится график утилизации cpu.
```

![alt text](https://github.com/mezhibo/monitoring-system/blob/fc3245198b6bd152a5f5e3be8775f0abead727d6/IMG/2.jpg)


И теперь, чисто для себя, ради эксперимента сделаем запрос на время отклика по http к базе influxdb 


![alt text](https://github.com/mezhibo/monitoring-system/blob/fc3245198b6bd152a5f5e3be8775f0abead727d6/IMG/3.jpg)




**Решение 9**


В ходе анализа, выяснил что плагин для докера уже прописан в докер компос файле

```
[[inputs.docker]]
  endpoint = "unix:///var/run/docker.sock"
```

Поэтому ничего добавлять и перезапукать не пришлось

И здесь видно что мы плагин длоя докера работает, и мы можем выбирать нужные метрики с докера


![alt text](https://github.com/mezhibo/monitoring-system/blob/25e48ae3dfcedd8bacdb8889e1e663ec1501f93e/IMG/4.jpg)


Также перейдем на главный дашборд, и видим что там тоже уже присутвуют эти графики

![alt text](https://github.com/mezhibo/monitoring-system/blob/25e48ae3dfcedd8bacdb8889e1e663ec1501f93e/IMG/5.jpg)

